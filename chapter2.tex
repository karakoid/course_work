\chapter{МЕТОДЫ ПОСТРОЕНИЯ MPC-РЕГУЛЯТОРОВ НА ОСНОВЕ МЕТОДОВ МАШИННОГО ОБУЧЕНИЯ}\label{chap2}

	
	В настоящей главе ознакомимся с одним из методов машинного обучения --- обучением с подкреплением, опишем базовые принципы его работы. Рассмотрим методы построения MPC-регуляторов на основе методов машинного обучения.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Обучение с подкреплением}\label{2sec:RL}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	
	Рассмотрим один из способов машинного обучения --- обучение с подкреплением (reinforcement learning). Суть этого метода обучения заключается в том, что наша система (агент) взаимодействует с некоторой средой, которая, в свою очередь, реагирует на действия агента, посылая сигналы подкрепления. Обучение с подкреплением является частным случаем обучения с учителем, однако главной особенностью RL является то, что учитель --- это среда или ее модель.\par
	Базовое обучение с подкреплением моделируется как процесс принятия решения Маркова:
\begin{itemize}
	\item набор состояний среды и агента S
	\item набор действий агента A
	\item $ P_a(s, s') = Pr(s_{t + 1} = s' | s_t = s, a_t = a) $ --- вероятность перехода из состояния $ s $ в состояние $ s' $ под действием агента $ a $
	\item $ R_a(s, s') $ --- награда за переход из состояния $ s $ в состояние $ s' $ под действием агента $ a $.
\end{itemize}
	Агент в каждый дискретный момент времени взаимодействует со средой. У агента имеется либо полный, либо частичный доступ к просмотру множества состояний. После действия, которое может быть произведено случайным образом, агент получает награду за выбранное им действие. Цель обучения с подкреплением связана с получением наибольшего количества наград. Выбор действия агентом может основываться не только на награде в данный момент времени, но и на награде, которую он получит после выбора этого действия. Это говорит о том, что обучение с подкреплением хорошо вписывается в задачи с долгосрочной перспективой.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Методы построения MPC-регуляторов на основе методов машинного обучения}\label{2sec:RL methods} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


	Существует некоторое количество принципов использования методов машинного обучения в MPC:
\begin{itemize}
	\item Явный MPC
	
	\item Аппроксимация закона управления
	
	\item Использование обучаемой модели для аппроксимации динамики прогнозирующей модели
	
	\item Итерационный подход для построения терминального региона и функции из предыдущих итераций.
\end{itemize}
	Рассмотрим явный MPC. Для линейных систем задача оптимизации может быть решена до начала процедуры управления. В результате решения получаем явный закон управления $ u(x) $.
	Необходимо найти $ x(t) $ в момент времени $ t $ в задаче вида
\begin{align}
	&V(x) = \min_{u(\cdot | t)} \sum_{k = t}^{t + N - 1} L(x(k | t), u(k | t)) + F(x(t + N | t)), \\
	&x(k + 1 | t) = Ax(k | t) + Bu(k | t), \; t \leq k \leq t + N - 1, \\
	&x(t | t) = x(t), \; t \leq k \leq t + N - 1, \\
	&C_x x(k | t) \leq d_x, \; t \leq k \leq t + N - 1, \\
	&C_u u(k | t) \leq d_u, \; t \leq k \leq t + N - 1, \\
	&C^f x(t + N | t) \leq d_f,
\end{align}
где $ L(x(k | t), u(k | t)) $ и $ F(x(k + N | t)) $ квадратичные функция стоимости и терминальная функция соответственно, имеющие вид
\begin{align}
	&L(x(k | t), u(k | t)) = x(t)^T Q x(t) + u(t)^T R u(t), \; Q, R > 0, \\
	&F(x(k + N | t)) = x(t)^T P x(t).
\end{align}
	Перепишем задачу (2.1) - (2.6) в матричном виде:
\begin{align}
	&\min_U \frac{1}{2} U^T H U + x^T F U + \frac{1}{2} x^T Y x, \\
	&GU \leq W + Ex(t)
\end{align} \par
$ Y = 2(Q + \Omega^T \tilde{Q} \Omega), \; H = 2(\Gamma^T \tilde{Q} \Gamma + \tilde{R}), \; F = 2 \Omega^T \tilde{Q} \Gamma $, \par
$ \tilde{Q} = diag(Q, ..., Q, P) \in \mathbb{R}^{n \times (N + 1)} , \; \tilde{R} = diag(R, ..., R) \in \mathbb{R}^{m \times N}$, \par
$ \Omega =
\begin{bmatrix}
	A \\
	A^2 \\
	\vdots \\
	A^N
\end{bmatrix}
 , \; \Gamma = 
 \begin{bmatrix}
 	B & 0 & & \ldots & 0 \\
 	AB & B & 0 &\ldots & 0 \\
 	\ldots \\
 	A^{N-1}B & A^{N-2}B & & \ldots & 0
 \end{bmatrix}
 $, \par
 $ G = diag(C_x, \ldots, C_x)\Gamma, \; W = [d_x, \ldots, d_x]^T, \; E = diag(C_x, \ldots, C_x)\Omega $. \par
	Сделав замену переменных вида \par $ z = U + H^{-1}F^Tx $, уравнения (2.9) - (2.10) примут вид
\begin{align}
	&\min_z \frac{1}{2} z^T H z + \frac{1}{2} x^T \tilde{Y} x, \\
	&Gz \leq W + Sx, \\
	&\tilde{Y} = Y - F H^{-1} F^T, \\
	&S = E + G H^{-1} F^T.
\end{align}
	Далее эта задача решается с помощью теории выпуклой оптимизации через условия Каруша-Куна-Такера.\par
	Явный MPC решает задачу для всех состояний, разбивая все пространство состояний на области, в каждой из которых находится своя явная функция управления. \par
	Алгоритм нахождения явных функций управления:
\begin{enumerate}
	\item Взять любой $ x_0 \in \mathbb{X} $
	\item Решить задачу (2.11) - (2.12) с начальным условием $ x = x_0 $
	\item Определить активные ограничения для оптимизационной задачи (2.11) - (2.12)
	\item Вычислить критическую область по активным ограничеям и вычислить функцию управления для этой области
	\item Перейти к новому $ x_0 $
\end{enumerate}


\section{Выводы}


	Каждый из методов имеет свое собственное применение в определенной ситуации. Рассмотренный метод имеет одним большим недостатком --- потенциально большим количеством областей, что может плохо сказаться на производительности метода. Однако для линейных задач с небольшим количеством областей явный MPC будет подходить.


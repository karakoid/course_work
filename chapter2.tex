 \chapter{МЕТОДЫ ПОСТРОЕНИЯ MPC-РЕГУЛЯТОРОВ НА ОСНОВЕ МЕТОДОВ МАШИННОГО ОБУЧЕНИЯ}\label{chap2}

	
	В настоящей главе будет рассмотрен метод машинного обучения~---~обучением с подкреплением, опишем базовые принципы его работы. Рассмотрим методы построения MPC-регуляторов на основе методов машинного обучения.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Обучение с подкреплением}\label{2sec:RL}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	
	Рассмотрим один из способов машинного обучения --- обучение с подкреплением (reinforcement learning). Суть этого метода обучения заключается в том, что система (агент) взаимодействует с некоторой средой, которая, в свою очередь, реагирует на действия агента, посылая сигналы подкрепления. Обучение с подкреплением является частным случаем обучения с учителем, однако главной особенностью RL является то, что учитель --- это среда или ее модель.\par
	Базовое обучение с подкреплением моделируется как процесс принятия решения Маркова:
\begin{itemize}
	\item набор состояний среды и агента S,
	\item набор действий агента A,
	\item $ P_a(s, s') = Pr(s_{t + 1} = s'| \, s_t = s, \, a_t = a) $ --- 					вероятность перехода из состояния $ s $ в состояние $ s' $ под действием 			агента $ a $,
	\item $ R_a(s, s') $ --- награда за переход из состояния $ s $ в состояние $ 			s' $ под действием агента $ a $.
\end{itemize}
	Агент в каждый дискретный момент времени взаимодействует со средой. У агента имеется либо полный, либо частичный доступ к просмотру множества состояний. После действия, которое может быть произведено случайным образом, агент получает награду за выбранное им действие. Цель обучения с подкреплением связана с получением наибольшего количества наград. Выбор действия агентом может основываться не только на награде в данный момент времени, но и на награде, которую он получит после выбора этого действия. Это говорит о том, что обучение с подкреплением хорошо вписывается в задачи с долгосрочной перспективой.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Методы построения MPC-регуляторов на основе методов машинного обучения}\label{2sec:RL methods} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


	Существует некоторое количество принципов использования методов машинного обучения в MPC:
\begin{itemize}
	\item Явный MPC
	
	\item Аппроксимация закона управления
	
	\item Использование обучаемой модели для аппроксимации динамики прогнозирующей модели
	
	\item Итерационный подход для построения терминального региона и функции из предыдущих итераций.
\end{itemize}
	Рассмотрим явный MPC. Для линейных систем задача оптимизации может быть решена до начала процедуры управления. В результате решения получаем явный закон управления $ u(x) $.
	Необходимо найти $ x(t) $ в момент времени $ t $ в задаче вида
\begin{equation}\label{2sec:mpc_problem}
	\begin{aligned}
		&V(x) = \min_{u(\cdot | t)} \sum_{k = t}^{t + N - 1} L(x(k| \, t),\, 					u(k| \, t)) + F(x(t + N| \, t)), \\
		&x(k + 1| \, t) = Ax(k| \, t) + Bu(k| \, t), \quad t \leq k 							\leq t + N - 1, \\
		&x(t| \, t) = x(t), \quad t \leq k \leq t + N - 1, \\
		&C_x x(k| \, t) \leq d_x, \quad t \leq k \leq t + N - 1, \\
		&C_u u(k| \, t) \leq d_u, \quad t \leq k \leq t + N - 1, \\
		&C^f x(t + N| \, t) \leq d_f,
	\end{aligned}
\end{equation}
где $ L(x(k| \, t), u(k| \, t)) $ и $ F(x(k + N| \, t)) $ квадратичные функция стоимости и терминальная функция соответственно, имеющие вид
\begin{align*}
	&L(x(k| \, t), u(k| \, t)) = x(t)^T Q x(t) + u(t)^T R u(t), \quad Q, 					R > 0, \\
	&F(x(k + N| \, t)) = x(t)^T P x(t).
\end{align*}
	Перепишем задачу \eqref{2sec:mpc_problem} в матричном виде:
\begin{equation}\label{2sec:mpc_problem_matrix}
	\begin{aligned}
		&\min_U \frac{1}{2} U^T H U + x^T F U + \frac{1}{2} x^T Y x, \\
		&GU \leq W + Ex(t)
	\end{aligned}
\end{equation}
\begin{align*}
	&Y = 2(Q + \Omega^T \tilde{Q} \Omega), \quad H = 2(\Gamma^T \tilde{Q} \Gamma 			+ \tilde{R}), \quad F = 2 \Omega^T \tilde{Q} \Gamma, \\
	&G = diag(C_x, \; \ldots \; , C_x)\Gamma, \; W = [d_x, \; \ldots \; , d_x]^T, 		\; E = diag(C_x, \; \ldots \; , C_x)\Omega, \\
	&\tilde{Q} = diag(Q, \; ... \; , Q, P) \in \mathbb{R}^{n \times (N + 1)} , 				\quad \tilde{R} = diag(R, \; ... \;, R) \in \mathbb{R}^{m \times N}, \\
	&\Omega =
			\begin{bmatrix}
				A \\
				A^2 \\
				\vdots \\
				A^N
			\end{bmatrix}
 		, \quad 
 		\Gamma = 
 			\begin{bmatrix}
 				B & 0 & 0 & \ldots & 0 \\
 				AB & B & 0 &\ldots & 0 \\
 				 & \ldots & \ldots \\
 				A^{N-1}B & A^{N-2}B & A^{N-3}B & \ldots & 0
 			\end{bmatrix}.
\end{align*}

	Сделав замену переменных вида 
\begin{align*}
	z = U + H^{-1}F^Tx,
\end{align*}
уравнения \eqref{2sec:mpc_problem_matrix} примут вид

\begin{equation}\label{2sec:mpc_problem_final}
	\begin{aligned}
		&\min_z \frac{1}{2} z^T H z + \frac{1}{2} x^T \tilde{Y} x, \\
		&Gz \leq W + Sx, \\
		&\tilde{Y} = Y - F H^{-1} F^T, \\
	&S = E + G H^{-1} F^T.
	\end{aligned}
\end{equation}	

	Далее задача \eqref{2sec:mpc_problem_final} решается с помощью теории выпуклой оптимизации через условия Каруша-Куна-Такера.\par
	Явный MPC решает задачу для всех состояний, разбивая все пространство состояний на области, в каждой из которых находится своя явная функция управления. \par
	Алгоритм нахождения явных функций управления:
\begin{enumerate}
	\item Взять любой $ x_0 \in \mathbb{X} $
	\item Решить задачу \eqref{2sec:mpc_problem_final} с начальным условием $ x = 		x_0 $
	\item Определить активные ограничения для оптимизационной задачи 						\eqref{2sec:mpc_problem_final}
	\item Вычислить критическую область по активным ограничеям и вычислить 					функцию управления для этой области
	\item Перейти к новому $ x_0 $
\end{enumerate}


\section{Выводы}


	Каждый из методов имеет свое собственное применение в определенной ситуации. Рассмотренный метод имеет одним большим недостатком --- потенциально большим количеством областей, что может плохо сказаться на производительности метода. Однако для линейных задач с небольшим количеством областей явный MPC будет подходить.

